{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks have become the defining model of deep learning. Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form."
      ],
      "metadata": {
        "id": "pTBmdbAmC8U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Linear Unit\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/mfOlDR6.png\">\n",
        "<p style=\"text-align: center;\">The Linear Unit:  y=wx+b</p>\n",
        "\n",
        "The input is x. Its connection to the neuron has a weight which is w. Whenever a value flows through a connection, you multiply the value by the connection's weight. For the input x, what reaches the neuron is w * x. <b>A neural network \"learns\" by modifying its weights.</b><br>\n",
        "\n",
        "The b is a special kind of weight we call the bias. The bias doesn't have any input data associated with it; instead, we put a 1 in the diagram so that the value that reaches the neuron is just b (since 1 * b = b). <b>The bias enables the neuron to modify the output independently of its inputs.</b> [বায়াস নিউরনকে তার ইনপুট থেকে স্বাধীনভাবে আউটপুট পরিবর্তন করতে সক্ষম করে।]<br>\n",
        "\n",
        "The y is the value the neuron ultimately outputs. To get the output, the neuron sums up all the values it receives through its connections. This neuron's activation is y = w * x + b, or as a formula  y=wx+b\n",
        " ."
      ],
      "metadata": {
        "id": "n6sIgBlPDF5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - The Linear Unit as a Model\n",
        "Individual neurons will usually only function as part of a larger network, it's often useful to start with a single neuron model as a baseline. Single neuron models are linear models.<br>\n",
        "Let's think about how this might work on a dataset like 80 Cereals. Training a model with 'sugars' (grams of sugars per serving) as input and 'calories' (calories per serving) as output, we might find the bias is b=90 and the weight is w=2.5. We could estimate the calorie content of a cereal with 5 grams of sugar per serving like this:\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/yjsfFvY.png\">\n",
        "<p>Computing with the linear unit.</p>\n",
        "And, checking against our formula, we have  calories=2.5×5+90=102.5 , just like we expect."
      ],
      "metadata": {
        "id": "5BSkOOADDcON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Inputs\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/vyXSnlZ.png\">\n",
        "<p>A linear unit with three inputs.</p>\n",
        "The formula for this neuron would be  y=w0x0+w1x1+w2x2+b\n",
        " . A linear unit with two inputs will fit a plane, and a unit with more inputs than that will fit a hyperplane."
      ],
      "metadata": {
        "id": "h290T-CYDi-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Units in Keras\n",
        "The easiest way to create a model in Keras is through keras.Sequential, which creates a neural network as a stack of layers.<br>\n",
        "We could define a linear model accepting three input features ('sugars', 'fiber', and 'protein') and producing a single output ('calories') like so:<br>"
      ],
      "metadata": {
        "id": "_sSsEVcoDsQ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zl5_f6YRCHze"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a network with 1 linear unit\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=1, input_shape=[3])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the first argument, units, we define how many outputs we want. In this case we are just predicting 'calories', so we'll use units=1.\n",
        "\n",
        "With the second argument, input_shape, we tell Keras the dimensions of the inputs. Setting input_shape=[3] ensures the model will accept three features as input ('sugars', 'fiber', and 'protein').\n",
        "\n",
        "This model is now ready to be fit to training data!"
      ],
      "metadata": {
        "id": "82A-EKuMDwCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers\n",
        "Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.<br>\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/2MA4iMV.png\">\n",
        "<p>A dense layer of two linear units receiving two inputs and a bias.</p>\n"
      ],
      "metadata": {
        "id": "ys2kQ4KWL4wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Activation Function\n",
        "Two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/OLSUEYT.png\">\n",
        "<p>Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions.</p>\n",
        "An activation function is simply some function we apply to each of a layer's outputs (its activations). The most common is the rectifier function max(0,x)\n",
        ".\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/aeIyAlF.png\">\n",
        "<br>\n",
        "The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.<br>\n",
        "\n",
        "When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. (For this reason, it's common to call the rectifier function the \"ReLU function\".) Applying a ReLU activation to a linear unit means the output becomes max(0, w * x + b), which we might draw in a diagram like:\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/eFry7Yu.png\">\n",
        "<p>A rectified linear unit.</p>"
      ],
      "metadata": {
        "id": "ZRojmrwxNmUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking Dense Layers\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/Y5iwFQZ.png\">\n",
        "\n",
        "The layers before the output layer are sometimes called hidden since we never see their outputs directly.\n",
        "<br>\n",
        "Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.\n"
      ],
      "metadata": {
        "id": "gyP2hBVePZkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Sequential Models\n"
      ],
      "metadata": {
        "id": "f_aPe0X5QSgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # the hidden ReLU layers\n",
        "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    layers.Dense(units=3, activation='relu'),\n",
        "    # the linear output layer\n",
        "    layers.Dense(units=1),\n",
        "])"
      ],
      "metadata": {
        "id": "6HwIE9LWDvRp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Layers\n",
        "In this case, we can define the activation in its own Activation layer, like so:\n",
        "\n",
        "layers.Dense(units=8),<br>\n",
        "layers.Activation('relu')<br>\n",
        "This is completely equivalent to the ordinary way:<br> layers.Dense(units=8, activation='relu')."
      ],
      "metadata": {
        "id": "_WNH6Bava8vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Alternatives to ReLU\n",
        "There is a whole family of variants of the 'relu' activation -- 'elu', 'selu', and 'swish', among others -- all of which we can use in Keras. Sometimes one activation will perform better than another on a given task, so we could consider experimenting with activations as we develop a model."
      ],
      "metadata": {
        "id": "tutyy6ccbJ0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Loss Function\n",
        "We've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.\n",
        "\n",
        "The loss function measures the disparity between the the target's true value and the value the model predicts.\n",
        "\n",
        "Different problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\n",
        "\n",
        "A common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
        "\n",
        "The total MAE loss on a dataset is the mean of all these absolute differences.\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/VDcvkZN.png\">\n",
        "Besides MAE, other loss functions we might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\n",
        "\n",
        "During training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\n"
      ],
      "metadata": {
        "id": "EaS92_W1cxAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Optimizer - Stochastic Gradient Descent\n",
        "We've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\n",
        "\n",
        "We've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\n",
        "\n",
        "Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
        "\n",
        "<li>Sample some training data and run it through the network to make predictions.\n",
        "<li>Measure the loss between the predictions and the true values.\n",
        "<li>Finally, adjust the weights in a direction that makes the loss smaller.<br>\n",
        "Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/rFI1tIk.gif\">\n",
        "\n",
        "A minibatch(or often just \"batch\") is a subset of the training dataset that is used to calculate the gradient and update the model parameters during training. Minibatches are typically much smaller than the entire training dataset, which can make training more efficient and reduce the risk of overfitting, while a complete round of the training data is called an epoch. The number of epochs we train for is how many times the network will see each training example.<br><br>\n",
        "The animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. We can see that the loss gets smaller as the weights get closer to their true values."
      ],
      "metadata": {
        "id": "CrT0ooLUdIES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate and Batch Size\n",
        "Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
        "\n",
        "The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds.\n",
        "\n",
        "Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer."
      ],
      "metadata": {
        "id": "jUntUgNoh-GD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the Loss and Optimizer\n",
        "After defining a model, you can add a loss function and optimizer with the model's compile method:"
      ],
      "metadata": {
        "id": "cahH2c23joFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss = \"mae\",\n",
        ")"
      ],
      "metadata": {
        "id": "m--I6PDMbi1a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!"
      ],
      "metadata": {
        "id": "ag1pXEsvkVoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the model"
      ],
      "metadata": {
        "id": "0N9JCEkI0o_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(\n",
        "#     X_train, y_train,\n",
        "#     validation_data(X_valid,y_valid),\n",
        "#     batch_size = 256,\n",
        "#     epochs=10,\n",
        "# )"
      ],
      "metadata": {
        "id": "JMBL9vlj0wX_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - Red Wine Quality\n",
        "(practice it by yourself) <br>\n",
        "<a href=\"https://www.kaggle.com/code/ryanholbrook/stochastic-gradient-descent\">click here</a>"
      ],
      "metadata": {
        "id": "84-29hfol1Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Learning Curve\n",
        "When we train a model we've been plotting the loss on the training set epoch by epoch. To this we'll add a plot the validation data too. These plots we call the learning curves.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/tHiVFnM.png\" width=400px>\n",
        "<p>The validation loss gives an estimate of the expected error on unseen data.<p>"
      ],
      "metadata": {
        "id": "cclloHJNmItL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Underfitting and overfitting.\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/eUF6mfo.png\" width=500px>\n",
        "\n",
        "This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn't learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise."
      ],
      "metadata": {
        "id": "nIMd4omhsup9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Capacity\n",
        "A model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that our network is underfitting the data, we should try increasing its capacity.\n",
        "\n",
        "We can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset."
      ],
      "metadata": {
        "id": "a4n2nFo1tQTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "wider = keras.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "deeper = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "lw-FLjPKvW83"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early Stopping\n",
        "We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called early stopping.\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/eP0gppr.png\" width=400px>\n",
        "\n",
        "This ensures that the model won't continue to learn noise and overfit the data.\n",
        "\n",
        "Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set our training epochs to some large number (more than we'll need), and early stopping will take care of the rest.\n",
        "\n"
      ],
      "metadata": {
        "id": "ok93yWZ2uxz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Early Stopping\n",
        "In Keras, we include early stopping in our training through a callback. A callback is just a function we want run every so often while the network trains. The early stopping callback will run after every epoch.\n"
      ],
      "metadata": {
        "id": "AmbIiEC82Rcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "7yjFKXBN46ou"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parameters say: \"If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found.\""
      ],
      "metadata": {
        "id": "t2gM6n8Y7718"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "Dropout is a regularization technique for neural networks that prevents overfitting by randomly dropping out neurons during training. Dropout is a simple but effective technique that can be used with any type of neural network.\n",
        "\n",
        "Here's how dropout works:\n",
        "\n",
        "<li>During training, a random fraction of neurons in each layer are dropped out. This means that their outputs are set to zero and they do not contribute to the forward or backward pass.\n",
        "<li>The weights of the network are updated as usual, but only the neurons that are not dropped out are used.\n",
        "<li>This process is repeated for each epoch of training.<br>\n",
        "Dropout effectively regularizes neural networks by forcing them to learn features that are robust to the presence or absence of individual neurons. This helps to prevent the network from overfitting to the training data and to generalize better to unseen data."
      ],
      "metadata": {
        "id": "vkH1-xSI79jB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Dropout\n",
        "In Keras, the dropout rate argument rate defines what percentage of the input units to shut off. Put the Dropout layer just before the layer you want the dropout applied to:\n"
      ],
      "metadata": {
        "id": "6CGc9INIt2ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.Sequential([\n",
        "    # ...\n",
        "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n",
        "    layers.Dense(16),\n",
        "    # ...\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vubksnPbuQAp",
        "outputId": "49ec4483-c362-4fed-c645-440f0a2cf018"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7ff611295960>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization\n",
        " \"Batch normalization\" (or \"batchnorm\") can help correct training that is slow or unstable.<br>\n",
        " With neural networks, it's generally a good idea to put all of our data on a common scale, perhaps with something like scikit-learn's StandardScaler or MinMaxScaler.Features that tend to produce activations of very different sizes can make for unstable training behavior."
      ],
      "metadata": {
        "id": "fVKxCPJIvu9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Batch Normalization\n",
        "It seems that batch normalization can be used at almost any point in a network. we can put it after a layer..."
      ],
      "metadata": {
        "id": "5MJgZXRA5xy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers.Dense(16, activation='relu'),\n",
        "layers.BatchNormalization(),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZx5J3ji52cM",
        "outputId": "ce11e2e6-b7a8-4f90-cc9a-34649398d4d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<keras.layers.normalization.batch_normalization.BatchNormalization at 0x7ff61123ebf0>,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... or between a layer and its activation function:"
      ],
      "metadata": {
        "id": "F1kc6vvO7kGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers.Dense(16),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation('relu'),"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcUwvUaV7noZ",
        "outputId": "b04d93e4-192e-41aa-9dcc-015a6165a9a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<keras.layers.core.activation.Activation at 0x7ff61123fd90>,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we add it as the first layer of our network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn's StandardScaler."
      ],
      "metadata": {
        "id": "PrlA4Xsr776c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy and Cross-Entropy\n",
        "Accuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: accuracy = number_correct / total. A model that always predicted correctly would have an accuracy score of 1.0.<br>\n",
        "\n",
        "The problem with accuracy (and most other classification metrics) is that it can't be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \"jumps\". So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.<br>\n",
        "Cross-entropy is a sort of measure for the distance from one probability distribution to another.\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/DwVV9bR.png\" width=400px>\n",
        "<p>The idea is that we want our network to predict the correct class with probability 1.0.</p>"
      ],
      "metadata": {
        "id": "fLM5AKR379SC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Probabilities with the Sigmoid Function\n",
        "The cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To covert the real-valued outputs produced by a dense layer into probabilities, we attach a new kind of activation function, the sigmoid activation.\n",
        "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/FYbRvJo.png\" width=400px>\n",
        "<p>The sigmoid function maps real numbers into the interval  [0,1]\n",
        " .</p>\n",
        "To get the final class prediction, we define a threshold probability. Typically this will be 0.5, so that rounding will give us the correct class: below 0.5 means the class with label 0 and 0.5 or above means the class with label 1. A 0.5 threshold is what Keras uses by default with its accuracy metric."
      ],
      "metadata": {
        "id": "FWDAmLbHDZr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation='relu', input_shape=[33]),\n",
        "    layers.Dense(4, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid'), #here the activation function is sigmoid\n",
        "])"
      ],
      "metadata": {
        "id": "lEagRphkLS4H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "d0VcjnU9D9Aj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verbose=0 means\n",
        "hiding the output because we have so many epochs"
      ],
      "metadata": {
        "id": "ZTehCn-9FGf-"
      }
    }
  ]
}